{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System for Survey Variable Resolution\n",
    "\n",
    "This Jupyter Notebook is designed to implement a RAG (Retrieval-Augmented Generation) system. The primary purpose of this system is to resolve user-submitted survey variable data and find other variables that are likely to match.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The RAG system leverages the power of retrieval-based and generative models for machine learning. It uses a two-step process:\n",
    "\n",
    "1. **Retrieval**: The system retrieves relevant documents (in this case, survey variables) from a Pinecone DB knowledge source based on the user-submitted data.\n",
    "\n",
    "2. **Generation**: The system then uses the retrieved documents to generate a response using a Cohere Reranking LLM and OpenAI's ChatOpenAI GPT 3.5 model. This response includes other variables that have a high likelihood of matching the user-submitted data.\n",
    "\n",
    "## Usage\n",
    "\n",
    "To use this notebook, input your survey variable data using the `input_data` directory. The RAG system will process this data, retrieve relevant variables from the knowledge source, and generate a list of variables that are likely to match your input.\n",
    "\n",
    "## Benefits\n",
    "\n",
    "The RAG system provides a powerful tool for survey data analysis. It can help identify patterns and correlations in the data, which can be invaluable for research and decision-making.\n",
    "\n",
    "Please note that the accuracy of the system's output depends on the quality and comprehensiveness of the knowledge source. Therefore, it's crucial to continually update and expand the knowledge source to improve the system's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "import cohere\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "\n",
    "\n",
    "from getpass import getpass\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY') or getpass(\"COHERE API Key: \")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Enter your Pinecone API key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\n",
    "--- MODEL CONFIG ---\n",
    "\"\"\"\n",
    "\n",
    "PINECONE_INDEX = \"biolincc-labels-001\" # Name of the Pinecone index\n",
    "TEMPERATURE = 0\n",
    "VECTOR_TEXT_FIELD = \"text\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "\"\"\"\"\n",
    "--- USER SUBMITTED VARIABLE CONFIG ---\n",
    "\"\"\"\n",
    "\n",
    "USER_SUBMITTED_VARIABLE_COL = \"name\"\n",
    "USER_SUBMITTED_LABEL_COL = \"description\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Vector DB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 259880}},\n",
       " 'total_vector_count': 259880}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = PINECONE_INDEX\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "  raise Exception(\"Pinecone index does not exist\")\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_community/vectorstores/pinecone.py:75: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "vectorstore = Pinecone(index, embed.embed_query, VECTOR_TEXT_FIELD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = 'input_data/test_data_dictionary.csv'\n",
    "\n",
    "# Read the CSV file and convert it into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample/Subject Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>336.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7418.211310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3661.967493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4388.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10370.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12240.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sample/Subject Count\n",
       "count            336.000000\n",
       "mean            7418.211310\n",
       "std             3661.967493\n",
       "min               12.000000\n",
       "25%             4388.000000\n",
       "50%            10122.000000\n",
       "75%            10370.000000\n",
       "max            12240.000000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(5)\n",
    "# The column we need to input into the RAG is 'description'\n",
    "# While in test mode this is all we want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_variables = df['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Generation Prompt**\n",
    "```\n",
    "    You are a data curator whos role is to harmonize biological variables in the NHLBI (National Heart Lung and Blood) data repository. \n",
    "    You are tasked with evaluating input variables from data dictionary that describes new data that will be added to the existing pool of variables in the repository.\n",
    "    For each new variable, a vector search engine has returned the three nearest existing variables found in the data repository. \n",
    "    Your job is determine which of these existing variables, if any, is the best fit for harmonizing the new variable to the existing variable given your understanding of the underlying scientific principles.\n",
    "    You must rationalize why the selected existing variable was chosen over the others, provide context to the relevancy of each existing variable to the new variable.\n",
    "    Then you are to provide the user with as much information as we can on how they can align their new variable with the selected existing variable.\n",
    "    \n",
    "    When there is no obvious match provide additional context for why you can't make a determination.\n",
    "    \n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Original question: {query}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "output_parser = LineListOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Generation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "    You are a data curator whos role is to harmonize biological variables in the NHLBI (National Heart Lung and Blood) data repository. \n",
    "    You are tasked with evaluating input variables from data dictionary that describes new data that will be added to the existing pool of variables in the repository.\n",
    "    For each new variable, a vector search engine has returned the three nearest existing variables found in the data repository. \n",
    "    Your job is determine which of these existing variables, if any, is the best fit for harmonizing the new variable to the existing variable given your understanding of the underlying scientific principles.\n",
    "    You must rationalize why the selected existing variable was chosen over the others, provide context to the relevancy of each existing variable to the new variable.\n",
    "    Then you are to provide the user with as much information as we can on how they can align their new variable with the selected existing variable.\n",
    "    \n",
    "    When there is no obvious match provide additional context for why you can't make a determination.\n",
    "    \n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Original question: {query}\n",
    "\"\"\"\n",
    "\n",
    "GEN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template=template,\n",
    ")\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Chain\n",
    "gen_chain = LLMChain(llm=llm, prompt=GEN_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Prompt Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IS THIS NEEDED? TBD\n",
    "# query_template = \"\"\"\n",
    "#     You are a data curator whos role is to harmonize biological variables in the NHLBI (National Heart Lung and Blood) data repository. \n",
    "#     You are tasked with evaluating input variables from data dictionary that describes new data that will be added to the existing pool of variables in the repository.\n",
    "    \n",
    "#     Original query: {query}\n",
    "# \"\"\"\n",
    "\n",
    "# QUERY_PROMPT = PromptTemplate(\n",
    "#     # input_variables=[\"question\"],\n",
    "#     input_variables=[\"query\"],\n",
    "#     template=query_template,\n",
    "# )\n",
    "# llm = ChatOpenAI(temperature=TEMPERATURE, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# # Chain\n",
    "# llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohere Rerank Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain_community.llms import Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_rerank = CohereRerank(cohere_api_key=COHERE_API_KEY)\n",
    "retriever=ContextualCompressionRetriever(\n",
    "    base_compressor=cohere_rerank, \n",
    "    base_retriever=vectorstore.as_retriever() \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting RAG elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_transform(inputs: dict) -> dict:\n",
    "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
    "    metadata = pd.DataFrame([d.metadata for d in docs], columns=['uid', 'relevance_score'])\n",
    "    docs = [d.page_content for d in docs]\n",
    "    print(docs)\n",
    "    docs_dict = {\n",
    "        \"query\": inputs[\"question\"],\n",
    "        \"contexts\": \"\\n---\\n\".join(docs),\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    return docs_dict\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\", \"metadata\"],\n",
    "    transform=retrieval_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, gen_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\", \"metadata\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mapped_vars(out, user_var):\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"user_variable\": user_var,\n",
    "            \"user_description\": out[\"query\"],\n",
    "            \"uid\": out[\"metadata\"][\"uid\"],\n",
    "            \"relevance_score\": out[\"metadata\"][\"relevance_score\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_llm_res_df(out, user_var):\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"user_variable\": user_var,\n",
    "            \"user_description\": out[\"query\"],\n",
    "            \"llm_response\": out[\"text\"]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_rag_chain(df, mapped_vars_df, llm_res_df):\n",
    "    for index, row in df.iterrows():\n",
    "        question = row[USER_SUBMITTED_LABEL_COL]\n",
    "        print(question)\n",
    "        out = rag_chain({\"question\": question})\n",
    "        mapped_vars_df = pd.concat([mapped_vars_df, format_mapped_vars(out, row[USER_SUBMITTED_VARIABLE_COL])])\n",
    "        llm_res_df = pd.concat([llm_res_df, format_llm_res_df(out, row[USER_SUBMITTED_VARIABLE_COL])])\n",
    "    return mapped_vars_df, llm_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map to orginal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_vars_df = pd.DataFrame(columns=[\n",
    "    \"user_variable\", # lowest\n",
    "    \"user_description\", # out[query] i.e. middle\n",
    "    \"uid\", # highest out[metadata][uid]\n",
    "    \"relevance_score\",  # highest out[metadata][uid]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_res_df = pd.DataFrame(columns=[\n",
    "    \"user_variable\", # lowest\n",
    "    \"user_description\", # out[query] i.e. middle\n",
    "    \"llm_response\", # out[text] i.e. middle\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consent group as determined by DAC\n",
      "['CONSENT RESEARCH SAMPLE', 'CONSENT STATUS', 'CONSENT SIGNED BY PARTICIPANT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/lqj94hqs2js64_82jtssv38h0000gn/T/ipykernel_34716/102934794.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  mapped_vars_df = pd.concat([mapped_vars_df, format_mapped_vars(out, row[USER_SUBMITTED_VARIABLE_COL])])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source repository where subjects originate\n",
      "['SOURCE FILE', 'SOURCE', 'SUBJECT LOCATION']\n",
      "Subject ID used in the Source Repository\n",
      "['SUBJECT_ID', 'SUBJECT ID', 'UNIQUE SUBJECT ID']\n",
      "Affection status\n",
      "['AFFECTIONATE - SCORE', 'AFFECTIONATE SUPPORT (0-100)', 'BASELINE ESSA3:LOVE, AFFECTION AVAILABLE']\n",
      "Source repository where samples originate\n",
      "['HAVE ORIGINAL SAMPLES FOR TESTING', 'SOURCE', 'SOURCE FILE']\n"
     ]
    }
   ],
   "source": [
    "mapped_vars_df, llm_res_df = execute_rag_chain(df, mapped_vars_df, llm_res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_variable</th>\n",
       "      <th>user_description</th>\n",
       "      <th>uid</th>\n",
       "      <th>relevance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONSENT</td>\n",
       "      <td>Consent group as determined by DAC</td>\n",
       "      <td>52783.0</td>\n",
       "      <td>0.454563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CONSENT</td>\n",
       "      <td>Consent group as determined by DAC</td>\n",
       "      <td>52786.0</td>\n",
       "      <td>0.124745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONSENT</td>\n",
       "      <td>Consent group as determined by DAC</td>\n",
       "      <td>52785.0</td>\n",
       "      <td>0.066448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUBJECT_SOURCE</td>\n",
       "      <td>Source repository where subjects originate</td>\n",
       "      <td>216548.0</td>\n",
       "      <td>0.084342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUBJECT_SOURCE</td>\n",
       "      <td>Source repository where subjects originate</td>\n",
       "      <td>216503.0</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_variable                            user_description       uid  \\\n",
       "0         CONSENT          Consent group as determined by DAC   52783.0   \n",
       "1         CONSENT          Consent group as determined by DAC   52786.0   \n",
       "2         CONSENT          Consent group as determined by DAC   52785.0   \n",
       "0  SUBJECT_SOURCE  Source repository where subjects originate  216548.0   \n",
       "1  SUBJECT_SOURCE  Source repository where subjects originate  216503.0   \n",
       "\n",
       "   relevance_score  \n",
       "0         0.454563  \n",
       "1         0.124745  \n",
       "2         0.066448  \n",
       "0         0.084342  \n",
       "1         0.077100  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_vars_df.to_csv('llm_output/mapped_vars_df.csv', index=False)\n",
    "\n",
    "mapped_vars_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_variable</th>\n",
       "      <th>user_description</th>\n",
       "      <th>llm_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONSENT</td>\n",
       "      <td>Consent group as determined by DAC</td>\n",
       "      <td>(lines, [Based on the provided existing variab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUBJECT_SOURCE</td>\n",
       "      <td>Source repository where subjects originate</td>\n",
       "      <td>(lines, [Based on the context provided, the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOURCE_SUBJECT_ID</td>\n",
       "      <td>Subject ID used in the Source Repository</td>\n",
       "      <td>(lines, [Based on the provided contexts, the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFFECTION_STATUS</td>\n",
       "      <td>Affection status</td>\n",
       "      <td>(lines, [Based on the provided existing variab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAMPLE_SOURCE</td>\n",
       "      <td>Source repository where samples originate</td>\n",
       "      <td>(lines, [Nearest existing variables found in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_variable                            user_description  \\\n",
       "0            CONSENT          Consent group as determined by DAC   \n",
       "0     SUBJECT_SOURCE  Source repository where subjects originate   \n",
       "0  SOURCE_SUBJECT_ID    Subject ID used in the Source Repository   \n",
       "0   AFFECTION_STATUS                            Affection status   \n",
       "0      SAMPLE_SOURCE   Source repository where samples originate   \n",
       "\n",
       "                                        llm_response  \n",
       "0  (lines, [Based on the provided existing variab...  \n",
       "0  (lines, [Based on the context provided, the ne...  \n",
       "0  (lines, [Based on the provided contexts, the b...  \n",
       "0  (lines, [Based on the provided existing variab...  \n",
       "0  (lines, [Nearest existing variables found in t...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_res_df.to_csv('llm_output/llm_res_df.csv', index=False)\n",
    "\n",
    "\n",
    "llm_res_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
