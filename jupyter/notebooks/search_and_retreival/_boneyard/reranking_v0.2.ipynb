{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2-XDGL6Oi6h4"
   },
   "source": [
    "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
    "\n",
    "# LangChain Multi-Query for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "qi8B1fgywJzE"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   pinecone-client==3.0.0 \\\n",
    "#   langchain==0.1.1 \\\n",
    "#   langchain-community==0.0.13 \\\n",
    "#   datasets==2.14.6 \\\n",
    "#   openai==1.6.1 \\\n",
    "#   tiktoken==0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in /opt/conda/lib/python3.11/site-packages (0.0.7)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.26 in /opt/conda/lib/python3.11/site-packages (from langchain-openai) (0.1.26)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-openai) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from langchain-openai) (1.12.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /opt/conda/lib/python3.11/site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (4.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (0.1.9)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.6.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.26->langchain-openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.26->langchain-openai) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.26->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.26->langchain-openai) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.0.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CPmfrdJ9_2YA"
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "S4Py-rVqx-I0"
   },
   "source": [
    "We will download an existing dataset from Hugging Face Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "iatOGmKgz8NE"
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Study</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AADR</td>\n",
       "      <td>AUGDATE_FZD</td>\n",
       "      <td>Augmentation Date, fzd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AADR</td>\n",
       "      <td>AUGDATE_FZD</td>\n",
       "      <td>Augmentation date, fzd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AADR</td>\n",
       "      <td>CENTER</td>\n",
       "      <td>Center where testing done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AADR</td>\n",
       "      <td>CLINIC</td>\n",
       "      <td>Clinic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AADR</td>\n",
       "      <td>CLINIC</td>\n",
       "      <td>Clinical Center</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Study     Variable                      Label\n",
       "0  AADR  AUGDATE_FZD     Augmentation Date, fzd\n",
       "1  AADR  AUGDATE_FZD     Augmentation date, fzd\n",
       "2  AADR       CENTER  Center where testing done\n",
       "3  AADR       CLINIC                     Clinic\n",
       "4  AADR       CLINIC            Clinical Center"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../raw_data/BIOLINCC_DEDUPDICTIONARY.csv', encoding='latin-1')\n",
    "df.rename(columns={'ï»¿Study': 'Study'}, inplace=True) # weird encoding issue...\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Study</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>472612</td>\n",
       "      <td>472604</td>\n",
       "      <td>388956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>240</td>\n",
       "      <td>381277</td>\n",
       "      <td>263317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>MASM</td>\n",
       "      <td>AGE</td>\n",
       "      <td>Count per minute with missing values for non-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>65563</td>\n",
       "      <td>207</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Study Variable                                              Label\n",
       "count   472612   472604                                             388956\n",
       "unique     240   381277                                             263317\n",
       "top       MASM      AGE  Count per minute with missing values for non-w...\n",
       "freq     65563      207                                               1440"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83664\n"
     ]
    }
   ],
   "source": [
    "total_missing_values = df.isna().sum().sum()\n",
    "print(total_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83656\n"
     ]
    }
   ],
   "source": [
    "missing_label = df['Label'].isna().sum()\n",
    "print(missing_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "P7E6JYtb0cW7"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    # Example output Pandas(Index=0, Study='AADR', Variable='AUGDATE_FZD', Label='Augmentation Date, fzd')\n",
    "    doc = Document(\n",
    "        page_content=row[3],  # replace \"row['Label']\" with \"row[1]\"\n",
    "        metadata={\n",
    "            \"Study\": row[1],  # replace \"row['Study']\" with \"row[2]\"\n",
    "            \"Variable\": row[2],  # replace \"row['Variable']\" with \"row[3]\"\n",
    "            \"id\": row[0],  # replace \"row['Index']\" with \"row[1]\"\n",
    "            \"text\": row[3]  # replace \"row['Label']\" with \"row[1]\"\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Study': 'AADR',\n",
       " 'Variable': 'AUGDATE_FZD',\n",
       " 'id': 0,\n",
       " 'text': 'Augmentation Date, fzd'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Augmentation Date, fzd'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yb540kEs_6PZ"
   },
   "source": [
    "## Embedding and Vector DB Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BlKEmBZMBxtd"
   },
   "source": [
    "Initialize our embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "qZ6vTiDPBznz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "# get openai api key from platform.openai.com\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IurEkeeI-IYl"
   },
   "source": [
    "Now we create our vector DB to store our vectors. For this we need to get a [free Pinecone API key](https://app.pinecone.io) — the API key can be found in the \"API Keys\" button found in the left navbar of the Pinecone dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92be5228-2d55-4d65-8f6a-33d15218fb34\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "import os \n",
    "\n",
    "print(os.getenv('PINECONE_API_KEY'))\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index, we set `dimension` equal to to dimensionality of Ada-002 (`1536`), and use a `metric` also compatible with Ada-002 (this can be either `cosine` or `dotproduct`). We also pass our `spec` to index initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "nL3KFF9E9Qb_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0xffff2a70c3d0>: Failed to resolve 'langchain-multi-query-demo-lp3qu8a.svc.apw5-4e34-81fa.pinecone.io' ([Errno -3] Temporary failure in name resolution)\")': /describe_index_stats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 388951}},\n",
       " 'total_vector_count': 388951}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain-multi-query-demo\"\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A3B7dHsd6QcP"
   },
   "source": [
    "Populate our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "B7Yi2YGBpTWf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388951"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "thfCYHuSpW4H"
   },
   "outputs": [],
   "source": [
    "# if you want to speed things up to follow along\n",
    "#docs = docs[:5000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8FbngTBzAAU-"
   },
   "source": [
    "## Multi-Query with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YVVYr13n_Ot2"
   },
   "source": [
    "Now we switch across to using our populated index as a vectorstore in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ETs0emsAh-K",
    "outputId": "0b1de24b-2f9f-48a6-d8ca-bd3d6aa007e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_community/vectorstores/pinecone.py:75: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "vectorstore = Pinecone(index, embed.embed_query, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "nW_GCB6a3_N_"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1iptBAriANrD"
   },
   "source": [
    "We initialize the `MultiQueryRetriever`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "yYjztBp2ANHC"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H8qZCd1TAMAn"
   },
   "source": [
    "We set logging so that we can see the queries as they're generated by our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "rgV1eYU6FgX7"
   },
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jrjwkpJWAaAn"
   },
   "source": [
    "To query with our multi-query retriever we call the `get_relevant_documents` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DJ4cSJXFinV",
    "outputId": "265900d1-6aa7-4d28-cbbe-e2e95b7df7b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the variables related to Asthma?', '2. Can you list all the variables associated with Asthma?', '3. Which variables are linked to the condition of Asthma?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"Asthma and COPD are both chronic lung diseases, but what are the differences between them\"\n",
    "variable_label = \"Asthma\"\n",
    "question = f\"Find all variables that match this {variable_label}\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query=question)\n",
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kSu1GsFfAqCd"
   },
   "source": [
    "From this we get a variety of docs retrieved by each of our queries independently. By default the `retriever` is returning `3` docs for each query — totalling `9` documents — however, as there is some overlap we actually return `6` unique docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce5WBh6MFltP",
    "outputId": "f7b06949-e2a6-472e-eaf9-e712dc4bcca2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Subject is classified as (1=Asthmatic, 2=Non-asthma', metadata={'Study': 'MCRBM', 'Variable': 'BPD_1000', 'id': 328131.0}),\n",
       " Document(page_content='ASTHMA SYMPTOMS1.How do you categorize your asthma', metadata={'Study': 'VIDA', 'Variable': 'AHA_1130', 'id': 459233.0}),\n",
       " Document(page_content='ASTHMA SYMPTOMS1.How do you categorize your asthma', metadata={'Study': 'SIENA', 'Variable': 'AHA_1130', 'id': 409970.0}),\n",
       " Document(page_content='ASTHMA SYMPTOMS1.How do you categorize your asthma', metadata={'Study': 'MCRBM', 'Variable': 'AHA_1130', 'id': 328032.0}),\n",
       " Document(page_content='ASTHMA SYMPTOMS1.How do you categorize your asthma', metadata={'Study': 'ALFA', 'Variable': 'AHA_1130', 'id': 9333.0}),\n",
       " Document(page_content='4. Thinking about all three asthma signs or symptom', metadata={'Study': 'PACT', 'Variable': 'SFD_1030', 'id': 367168.0}),\n",
       " Document(page_content='VIII.4 ASTHMA', metadata={'Study': 'CSSCD', 'Variable': 'ASTHMA', 'id': 122072.0}),\n",
       " Document(page_content='Monitored condition - asthma', metadata={'Study': 'HCHS', 'Variable': 'MHE2C', 'id': 214629.0}),\n",
       " Document(page_content='24.4.d.vi. Asthma', metadata={'Study': 'TOLSURF', 'Variable': 'ND3ASTHMA', 'id': 455837.0})]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject is classified as (1=Asthmatic, 2=Non-asthma',\n",
       " 'ASTHMA SYMPTOMS1.How do you categorize your asthma',\n",
       " 'ASTHMA SYMPTOMS1.How do you categorize your asthma',\n",
       " 'ASTHMA SYMPTOMS1.How do you categorize your asthma',\n",
       " 'ASTHMA SYMPTOMS1.How do you categorize your asthma',\n",
       " '4. Thinking about all three asthma signs or symptom',\n",
       " 'VIII.4 ASTHMA',\n",
       " 'Monitored condition - asthma',\n",
       " '24.4.d.vi. Asthma']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docs['page_content']\n",
    "type(docs[0])\n",
    "rerank_docs = [x.dict()['page_content'] for x in docs]\n",
    "rerank_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLMwfZPfBF89"
   },
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import json\n",
    "\n",
    "api_key = os.environ['COHERE_API_KEY']\n",
    "coh = cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Which variables are relevant for capturing information about heart disease?', '2. What are the key variables that can help in understanding heart disease?', '3. Can you identify the variables that are important for detecting heart disease?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rank: 1, Document Index: 0\n",
      "Document: Valvular heart disease INFO: baseline_variables\n",
      "Relevance Score: 0.43\n",
      "\n",
      "\n",
      "Document Rank: 2, Document Index: 3\n",
      "Document: Congenital heart disease INFO: baseline_variables\n",
      "Relevance Score: 0.40\n",
      "\n",
      "\n",
      "Document Rank: 3, Document Index: 1\n",
      "Document: 1a. Heart Disease\n",
      "Relevance Score: 0.06\n",
      "\n",
      "\n",
      "Document Rank: 4, Document Index: 4\n",
      "Document: 4. Coronary artery disease?\n",
      "Relevance Score: 0.06\n",
      "\n",
      "\n",
      "Document Rank: 5, Document Index: 7\n",
      "Document: 6. Coronary artery disease?\n",
      "Relevance Score: 0.05\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import json\n",
    "\n",
    "api_key = os.environ['COHERE_API_KEY']\n",
    "coh = cohere.Client(api_key)\n",
    "\n",
    "variable_label = \"heart disease\"\n",
    "# question = f\"Find all variables that match this {variable_label}\"\n",
    "question = f\"From the provided variables, find all that would be helpful in capturing information about {variable_label}\"\n",
    "docs = retriever.get_relevant_documents(query=question)\n",
    "rerank_docs = [x.dict()['page_content'] for x in docs]\n",
    "\n",
    "results = coh.rerank(query=question, documents=rerank_docs, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
    "# results = coh.rerank(query=question, documents=rerank_docs, top_n=5, model='rerank-multilingual-v2.0')\n",
    "# json.dumps(json.loads(results), indent=3)\n",
    "results\n",
    "\n",
    "\n",
    "for idx, r in enumerate(results):\n",
    "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
    "  print(f\"Document: {r.document['text']}\")\n",
    "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Which variables are relevant for capturing information about heart disease?', '2. What are the key variables that can help in understanding heart disease?', '3. Can you identify the variables that are important for detecting heart disease?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rank: 1, Document Index: 3\n",
      "Document: Congenital heart disease INFO: baseline_variables\n",
      "Relevance Score: 0.04\n",
      "\n",
      "\n",
      "Document Rank: 2, Document Index: 0\n",
      "Document: Valvular heart disease INFO: baseline_variables\n",
      "Relevance Score: 0.03\n",
      "\n",
      "\n",
      "Document Rank: 3, Document Index: 2\n",
      "Document: Does patient have evidence of CHD? (1 yes  2 no)\n",
      "Relevance Score: 0.00\n",
      "\n",
      "\n",
      "Document Rank: 4, Document Index: 1\n",
      "Document: 1a. Heart Disease\n",
      "Relevance Score: 0.00\n",
      "\n",
      "\n",
      "Document Rank: 5, Document Index: 4\n",
      "Document: 4. Coronary artery disease?\n",
      "Relevance Score: 0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "variable_label = \"heart disease\"\n",
    "# question = f\"Find all variables that match this {variable_label}\"\n",
    "question = f\"From the provided variables, find all that would be helpful in capturing information about {variable_label}\"\n",
    "docs = retriever.get_relevant_documents(query=question)\n",
    "rerank_docs = [x.dict()['page_content'] for x in docs]\n",
    "\n",
    "# results = coh.rerank(query=question, documents=rerank_docs, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
    "results = coh.rerank(query=question, documents=rerank_docs, top_n=5, model='rerank-multilingual-v2.0')\n",
    "# json.dumps(json.loads(results), indent=3)\n",
    "results\n",
    "\n",
    "\n",
    "for idx, r in enumerate(results):\n",
    "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
    "  print(f\"Document: {r.document['text']}\")\n",
    "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<stop here>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KLMwfZPfBF89"
   },
   "source": [
    "## Adding the Generation in RAG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "X79eNNL_BM4G"
   },
   "source": [
    "So far we've built a multi-query powered **R**etrieval **A**ugmentation chain. Now, we need to add **G**eneration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "jNnXYOtqypiz"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template=\"\"\"You are a helpful assistant who answers user queries using the\n",
    "    contexts provided. If the question cannot be answered using the information\n",
    "    provided say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Question: {query}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "h6GVEZkhytdM",
    "outputId": "f03086b8-8d30-4d6e-a723-833ffecbcf8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = qa_chain(\n",
    "    inputs={\n",
    "        \"query\": question,\n",
    "        \"contexts\": \"\\n---\\n\".join([d.page_content for d in docs])\n",
    "    }\n",
    ")\n",
    "out[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KemgDCg8DkgE"
   },
   "source": [
    "## Chaining Everything with a SequentialChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kTbLlWgEEII1"
   },
   "source": [
    "We can pull together the logic above into a function or set of methods, whatever is prefered — however if we'd like to use LangChain's approach to this we must \"chain\" together multiple chains. The first retrieval component is (1) not a chain per se, and (2) requires processing of the output. To do that, and fit with LangChain's \"chaining chains\" approach, we setup the _retrieval_ component within a `TransformChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BpFmiRtYDpHp"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "def retrieval_transform(inputs: dict) -> dict:\n",
    "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
    "    docs = [d.page_content for d in docs]\n",
    "    docs_dict = {\n",
    "        \"query\": inputs[\"question\"],\n",
    "        \"contexts\": \"\\n---\\n\".join(docs)\n",
    "    }\n",
    "    return docs_dict\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SoD45Au1Eg-r"
   },
   "source": [
    "Now we chain this with our generation step using the `SequentialChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "azqCwDwXEkDT"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xpB2aWV4ESzf"
   },
   "source": [
    "Then we perform the full RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "JvJbUaLqFRG2",
    "outputId": "582caa21-777a-4a01-a618-9db64185ad5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What information can you provide about llama 2?', '2. Could you give me some details about llama 2?', '3. I would like to learn more about llama 2. Can you help me with that?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. These LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. They have been shown to outperform open-source chat models on most benchmarks and are considered a suitable substitute for closed-source models based on humane evaluations for helpfulness and safety. The approach to fine-tuning and safety is described in detail in the work.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rag_chain({\"question\": question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bLmv01geK-ZS"
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vAZVPhHzLDQQ"
   },
   "source": [
    "## Custom Multiquery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rI-KVO6zjJZw"
   },
   "source": [
    "We'll try this with two prompts, both encourage more variety in search queries.\n",
    "\n",
    "**Prompt A**\n",
    "```\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives.\n",
    "Each query MUST tackle the question from a different viewpoint,\n",
    "we want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "```\n",
    "\n",
    "\n",
    "**Prompt B**\n",
    "```\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives. The user questions\n",
    "are focused on Large Language Models, Machine Learning, and related\n",
    "disciplines.\n",
    "Each query MUST tackle the question from a different viewpoint, we\n",
    "want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4IlEnYeKLFzh"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives. The user questions\n",
    "are focused on Large Language Models, Machine Learning, and related\n",
    "disciplines.\n",
    "Each query MUST tackle the question from a different viewpoint, we\n",
    "want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template,\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CgduNJWLBez",
    "outputId": "7ffee6c2-27b4-4bdf-8c79-7effd27e3cd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key features and capabilities of Large Language Model Llama 2?', '2. How does Llama 2 compare to other Large Language Models in terms of performance and efficiency?', '3. What are the applications and use cases of Llama 2 in the field of Machine Learning and Natural Language Processing?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "docs = retriever.get_relevant_documents(\n",
    "    query=question\n",
    ")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSySsaDKMK1i",
    "outputId": "e6f95abd-99fc-4576-d1f4-5fd4c21c70ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'chunk-id': '1', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='2\\n3.4.3 Even programmatic measures of model capability can be highly subjective . . . . . . . 15\\n3.5 Even large language models are brittle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.6 Social bias in large language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.7 Performance on non-English languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4 Behavior on selected tasks 21\\n4.1 Checkmate-in-one task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n4.2 Periodic elements task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n5 Additional related work 24\\n6 Discussion 25', metadata={'chunk-id': '14', 'id': '2206.04615', 'source': 'http://arxiv.org/pdf/2206.04615', 'title': 'Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models'}),\n",
       " Document(page_content='challenges described above) about how the development of large language models has unfolded thus far, including a\\nquantitative analysis of the increasing gap between academia and industry for large model development.\\nFinally, in Section 4 we outline policy interventions that may help concretely address the challenges we outline in\\nSections 2 and 3 in order to help guide the development and deployment of larger models for the broader social good.\\nWe leave some illustrative experiments, technical details, and caveats about our claims in Appendix A.\\n2 DISTINGUISHING FEATURES OF LARGE GENERATIVE MODELS\\nWe claim that large generative models (e.g., GPT-3 [ 11], LaMDA [ 78], Gopher [ 62], etc.) are distinguished by four\\nfeatures:\\n•Smooth, general capability scaling : It is possible to predictably improve the general performance of generative\\nmodels — their loss on capturing a specific, though very broad, data distribution — by scaling up the size of the\\nmodels, the compute used to train them, and the amount of data they’re trained on in the correct proportions.\\nThese proportions can be accurately predicted by scaling laws (Figure 1). We believe that these scaling laws\\nde-risk investments in building larger and generally more capable models despite the high resource costs and the\\ndifficulty of predicting precisely how well a model will perform on a specific task. Note, the harmful properties', metadata={'chunk-id': '9', 'id': '2202.07785', 'source': 'http://arxiv.org/pdf/2202.07785', 'title': 'Predictability and Surprise in Large Generative Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'chunk-id': '9', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='but BoolQ. Similarly, this model surpasses PaLM540B everywhere but on BoolQ and WinoGrande.\\nLLaMA-13B model also outperforms GPT-3 on\\nmost benchmarks despite being 10 \\x02smaller.\\n3.2 Closed-book Question Answering\\nWe compare LLaMA to existing large language\\nmodels on two closed-book question answering\\nbenchmarks: Natural Questions (Kwiatkowski\\net al., 2019) and TriviaQA (Joshi et al., 2017). For\\nboth benchmarks, we report exact match performance in a closed book setting, i.e., where the models do not have access to documents that contain\\nevidence to answer the question. In Table 4, we\\nreport performance on NaturalQuestions, and in Table 5, we report on TriviaQA. On both benchmarks,\\nLLaMA-65B achieve state-of-the-arts performance\\nin the zero-shot and few-shot settings. More importantly, the LLaMA-13B is also competitive on\\nthese benchmarks with GPT-3 and Chinchilla, despite being 5-10 \\x02smaller. This model runs on a\\nsingle V100 GPU during inference.\\n0-shot 1-shot 5-shot 64-shot\\nGopher 280B 43.5 - 57.0 57.2', metadata={'chunk-id': '17', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}),\n",
       " Document(page_content='5 Discussion 19\\n6 Conclusion 21\\n1 Introduction: motivation for the survey and deﬁnitions\\n1.1 Motivation\\nLarge Language Models (LLMs) ( Devlin et al. ,2019;Brown et al. ,2020;Chowdhery et al. ,2022) have fueled dramatic progress in Natural Language Processing (NLP ) and are already core in several products with\\nmillions of users, such as the coding assistant Copilot ( Chen et al. ,2021), Google search engine1or more recently ChatGPT2. Memorization ( Tirumala et al. ,2022) combined with compositionality ( Zhou et al. ,2022)\\ncapabilities made LLMs able to execute various tasks such as language understanding or conditional and unconditional text generation at an unprecedented level of pe rformance, thus opening a realistic path towards\\nhigher-bandwidth human-computer interactions.\\nHowever, LLMs suﬀer from important limitations hindering a broader deployment. LLMs often provide nonfactual but seemingly plausible predictions, often referr ed to as hallucinations ( Welleck et al. ,2020). This\\nleads to many avoidable mistakes, for example in the context of arithmetics ( Qian et al. ,2022) or within\\na reasoning chain ( Wei et al. ,2022c ). Moreover, many LLMs groundbreaking capabilities seem to emerge', metadata={'chunk-id': '5', 'id': '2302.07842', 'source': 'http://arxiv.org/pdf/2302.07842', 'title': 'Augmented Language Models: a Survey'}),\n",
       " Document(page_content='practicable options for academic research since they were acquired by Appen, a company that is\\nfocused on a business market.\\nThis paper explores the potential of large language models (LLMs) for text annotation tasks, with a\\nfocus on ChatGPT, which was released in November 2022. It demonstrates that zero-shot ChatGPT\\nclassiﬁcations (that is, without any additional training) outperform MTurk annotations, at a fraction\\nof the cost. LLMs have been shown to perform very well for a wide range of purposes, including\\nideological scaling (Wu et al., 2023), the classiﬁcation of legislative proposals (Nay, 2023), the\\nresolution of cognitive psychology tasks (Binz and Schulz, 2023), and the simulation of human\\nsamples for survey research (Argyle et al., 2023). While a few studies suggested that ChatGPT\\nmight perform text annotation tasks of the kinds we have described (Kuzman, Mozeti ˇc and Ljubeši ´c,\\n2023; Huang, Kwak and An, 2023), to the best of our knowledge our work is the ﬁrst systematic\\nevaluation. Our analysis relies on a sample of 6,183 documents, including tweets and news articles', metadata={'chunk-id': '3', 'id': '2303.15056', 'source': 'http://arxiv.org/pdf/2303.15056', 'title': 'ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F4q65OEiizU2"
   },
   "source": [
    "Putting this together in another `SequentialChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LTRjTIKzi2-g"
   },
   "outputs": [],
   "source": [
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Rda74xhpjE6A"
   },
   "source": [
    "And asking again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9UcBY71cjGgX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key features and capabilities of Large Language Model Llama 2?', '2. How does Llama 2 compare to other Large Language Models in terms of performance and efficiency?', '3. What are the applications and use cases of Llama 2 in the field of Machine Learning and Natural Language Processing?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. These models, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases and have been shown to outperform open-source chat models on most benchmarks. They are considered as a suitable substitute for closed-source models in terms of helpfulness and safety. The development of Llama 2 addresses challenges such as programmatic measures of model capability, brittleness of large language models, social bias, and performance on non-English languages.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rag_chain({\"question\": question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8jULksgk7gLA"
   },
   "source": [
    "After finishing, delete your Pinecone index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
