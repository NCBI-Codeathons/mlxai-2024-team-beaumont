{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Data Dictionary \n",
    "\n",
    "## Input\n",
    "\n",
    "Reranked RAG Results from Cohere Rerank model see docs => [Cohere Rerank Documentation](https://docs.cohere.com/reference/rerank-1)\n",
    "\n",
    "## Outcome\n",
    "\n",
    "Produces a dataframe of data that suggests variable options based on a user input data dictionary in phase 1.\n",
    "\n",
    "### Output Dataframe structure:\n",
    "\n",
    "First 3 columns are related to a row of user data: \n",
    "\n",
    "user_study | user_var | user_label \n",
    "\n",
    "The next columns are specific to the LLM Output. Cohere will return the top 3 values for each variable and each option will contain the following columns:\n",
    "\n",
    "| option_X_var | option_X_score | option_X_label | option_X_study\n",
    "\n",
    "#### Conditional output scenarios\n",
    "\n",
    "1) If one of the options returns a significantly high match and there are no other close competing options, then we will only return the first value\n",
    "2) If there are no good options we return the options AND prompt our LLM to return an explantion of why the three options are faulty\n",
    "3) If all/multiple options have a close significant match score we return all three options AND query a vector database on which variable's study most closely matches our current user submitted study (TBD)\n",
    "\n",
    "| Column 1 | Column 2 | Column 3 |\n",
    "| -------- | -------- | -------- |\n",
    "| Cell 1A  | Cell 2A  | Cell 3A  |\n",
    "| Cell 1B  | Cell 2B  | Cell 3B  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import cohere\n",
    "\n",
    "from getpass import getpass\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_29', 'id': 361188.0}),\n",
       " Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_15', 'id': 361187.0}),\n",
       " Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_8', 'id': 361189.0}),\n",
       " Document(page_content='Swine H1N1 Vaccination INFO: baseline_variables', metadata={'Study': 'H1N1', 'Variable': 'SWINE_H1N1_VACC', 'id': 208960.0}),\n",
       " Document(page_content='Who is the primary source of information?', metadata={'Study': 'SHEP', 'Variable': 'SH51_INFO_SOURCE', 'id': 407061.0}),\n",
       " Document(page_content='Abstracting for cohort or surveillance Q3', metadata={'Study': 'ARIC', 'Variable': 'HRAA03', 'id': 24922.0}),\n",
       " Document(page_content='Method of Data Collection', metadata={'Study': 'PROPPR', 'Variable': 'DOMOIVTR', 'id': 379440.0}),\n",
       " Document(page_content='Method of Data Collection', metadata={'Study': 'PROPPR', 'Variable': 'DOMOBLDT', 'id': 379439.0}),\n",
       " Document(page_content='COVID-19 specimen #1 type (e.g. nasopharyngeal swab', metadata={'Study': 'REDCORAL', 'Variable': 'CX_COVID1_SPECTYPE', 'id': 384005.0})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_29', 'id': 361188.0}),\n",
    " Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_15', 'id': 361187.0}),\n",
    " Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_8', 'id': 361189.0}),\n",
    " Document(page_content='Swine H1N1 Vaccination INFO: baseline_variables', metadata={'Study': 'H1N1', 'Variable': 'SWINE_H1N1_VACC', 'id': 208960.0}),\n",
    " Document(page_content='Who is the primary source of information?', metadata={'Study': 'SHEP', 'Variable': 'SH51_INFO_SOURCE', 'id': 407061.0}),\n",
    " Document(page_content='Abstracting for cohort or surveillance Q3', metadata={'Study': 'ARIC', 'Variable': 'HRAA03', 'id': 24922.0}),\n",
    " Document(page_content='Method of Data Collection', metadata={'Study': 'PROPPR', 'Variable': 'DOMOIVTR', 'id': 379440.0}),\n",
    " Document(page_content='Method of Data Collection', metadata={'Study': 'PROPPR', 'Variable': 'DOMOBLDT', 'id': 379439.0}),\n",
    " Document(page_content='COVID-19 specimen #1 type (e.g. nasopharyngeal swab', metadata={'Study': 'REDCORAL', 'Variable': 'CX_COVID1_SPECTYPE', 'id': 384005.0})]\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_label = \"covid\"\n",
    "# question = f\"Find all variables that match this {variable_label}\"\n",
    "question = f\"From the provided variables, find all that would be helpful in capturing information about {variable_label}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere Reranking\n",
    "\n",
    "This is the start of the data processing post `jupyter/notebooks/search_and_retreival/reranking.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['COHERE_API_KEY']\n",
    "coh = cohere.Client(api_key)\n",
    "\n",
    "rerank_docs = [x.dict()['page_content'] for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example cohere results n 3\n",
    "results = [\n",
    "    {\n",
    "      \"document\": {\n",
    "        \"text\": \"COVID-19 specimen #1 type (e.g. nasopharyngeal swab\"\n",
    "      },\n",
    "      \"index\": 2,\n",
    "      \"relevance_score\": 0.08\n",
    "    },\n",
    "    {\n",
    "      \"document\": {\n",
    "        \"text\": \"Was information on the COVID Ordinal Outcome Scale\"\n",
    "      },\n",
    "      \"index\": 0,\n",
    "      \"relevance_score\":  0.05\n",
    "    },\n",
    "    {\n",
    "      \"document\": {\n",
    "        \"text\": \"Abstracting for cohort or surveillance Q3\"\n",
    "      },\n",
    "      \"index\": 1,\n",
    "      \"relevance_score\":  0.03\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "CohereAPIError",
     "evalue": "invalid api token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCohereAPIError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcoh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrerank_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrerank-multilingual-v2.0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Rank: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Document Index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cohere/client.py:742\u001b[0m, in \u001b[0;36mClient.rerank\u001b[0;34m(self, query, documents, model, top_n, max_chunks_per_doc)\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CohereError(\n\u001b[1;32m    730\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid format for documents, must be a list of strings or dicts with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m key\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    733\u001b[0m json_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: parsed_docs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_chunks_per_doc\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_chunks_per_doc,\n\u001b[1;32m    740\u001b[0m }\n\u001b[0;32m--> 742\u001b[0m reranking \u001b[38;5;241m=\u001b[39m Reranking(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRERANK_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_body\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m reranking\u001b[38;5;241m.\u001b[39mresults:\n\u001b[1;32m    744\u001b[0m     rank\u001b[38;5;241m.\u001b[39mdocument \u001b[38;5;241m=\u001b[39m parsed_docs[rank\u001b[38;5;241m.\u001b[39mindex]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cohere/client.py:929\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, endpoint, json, files, method, stream, params)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m jsonlib\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mJSONDecodeError:  \u001b[38;5;66;03m# CohereAPIError will capture status\u001b[39;00m\n\u001b[1;32m    927\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError\u001b[38;5;241m.\u001b[39mfrom_response(response, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode json body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 929\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cohere/client.py:871\u001b[0m, in \u001b[0;36mClient._check_response\u001b[0;34m(self, json_response, headers, status_code)\u001b[0m\n\u001b[1;32m    869\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-API-Warning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_response:  \u001b[38;5;66;03m# has errors\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    872\u001b[0m         message\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    873\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    874\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    875\u001b[0m     )\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    878\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected client error (status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    880\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    881\u001b[0m     )\n",
      "\u001b[0;31mCohereAPIError\u001b[0m: invalid api token"
     ]
    }
   ],
   "source": [
    "# results = coh.rerank(query=question, documents=rerank_docs, top_n=10, model='rerank-multilingual-v2.0')\n",
    "\n",
    "# for idx, r in enumerate(results):\n",
    "#   print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
    "#   print(f\"Document: {r.document['text']}\")\n",
    "#   print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
    "#   print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the list of dictionaries based on 'relevance_score' in descending order\n",
    "sorted_results = sorted(results, key=lambda x: x['relevance_score'], reverse=True)\n",
    "\n",
    "# Get the top 3 results\n",
    "top_3_results = sorted_results[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Generation in RAG\n",
    "\n",
    "Needed To Dos:\n",
    "- Change the prompt template to more accurately support our end goal\n",
    "  - ... which is?\n",
    "\n",
    "Optional To Dos:\n",
    "- Play around with temperature\n",
    "- Select different OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below I am putting a user query and retrieved context. Read both inputs and make sure to output one score from 0.0 to 1.0 that indicates how relevant the retrieved context is to answer the user query. Query: covid Context: COVID-19 specimen #1 type (e.g. nasopharyngeal swab',\n",
       " 'Below I am putting a user query and retrieved context. Read both inputs and make sure to output one score from 0.0 to 1.0 that indicates how relevant the retrieved context is to answer the user query. Query: covid Context: Was information on the COVID Ordinal Outcome Scale',\n",
       " 'Below I am putting a user query and retrieved context. Read both inputs and make sure to output one score from 0.0 to 1.0 that indicates how relevant the retrieved context is to answer the user query. Query: covid Context: Abstracting for cohort or surveillance Q3']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Below I am putting a user query and retrieved context. Read both inputs and make sure to output one score from 0.0 to 1.0 that indicates how relevant the retrieved context is to answer the user query.\"\n",
    "relevancy_prompt_list = []\n",
    "for i in range(len(top_3_results)):\n",
    "    relevancy_prompt_list.append(f\"{s} Query: {variable_label} Context: {top_3_results[i]['document']['text']}\")\n",
    "relevancy_prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template = \"\"\"Human: Below I am putting a user query and retrieved context. Read both inputs and make sure and provide and explanation how relevant the retrieved context is to answer the user query.\n",
    "\n",
    "\n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Question: {query}\"\"\",\n",
    ")\n",
    "\n",
    "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19 specimen #1 type (e.g. nasopharyngeal swab\n",
      "---\n",
      "Was information on the COVID Ordinal Outcome Scale\n",
      "---\n",
      "Abstracting for cohort or surveillance Q3\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n---\\n\".join([d['document']['text'] for d in top_3_results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: COVID-19 specimen #1 type (e.g. nasopharyngeal swab\n",
      "Relevance Score: The retrieved context is directly related to the user query about COVID-19. It provides information about the type of specimen that is commonly used for COVID-19 testing, which is a nasopharyngeal swab. This information is relevant as it helps to understand the testing process for COVID-19 and the type of sample that is typically collected for diagnosis.\n",
      "\n",
      "\n",
      "Prompt: Was information on the COVID Ordinal Outcome Scale\n",
      "Relevance Score: The retrieved context is not directly relevant to the user query about \"covid.\" The context provided seems to be incomplete and does not provide enough information to determine its relevance to the user query. It would be helpful to have more context or a clearer connection to the user query in order to provide a relevant explanation.\n",
      "\n",
      "\n",
      "Prompt: Abstracting for cohort or surveillance Q3\n",
      "Relevance Score: The retrieved context \"Abstracting for cohort or surveillance\" does not seem directly relevant to the user query \"covid.\" However, it could potentially be relevant if the user is looking for information on how data is being collected and analyzed for COVID-19 surveillance or cohort studies. In this case, the context could provide insights into the methods and processes used for tracking and monitoring COVID-19 cases within specific populations.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in top_3_results:\n",
    "    print(f\"Prompt: {prompt['document']['text']}\")\n",
    "    print(f\"Relevance Score: {qa_chain(inputs={'query': variable_label, 'contexts': prompt['document']['text']})['text']}\")\n",
    "    print(\"\\n\")\n",
    "# out = qa_chain(\n",
    "#     inputs={\n",
    "#         \"query\": variable_label,\n",
    "#         \"contexts\": \"\\n---\\n\".join([d['document']['text'] for d in top_3_results])\n",
    "#     }\n",
    "# )\n",
    "# out[\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
