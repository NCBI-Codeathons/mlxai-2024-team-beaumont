{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Data Dictionary \n",
    "\n",
    "## Input\n",
    "\n",
    "Reranked RAG Results from Cohere Rerank model see docs => [Cohere Rerank Documentation](https://docs.cohere.com/reference/rerank-1)\n",
    "\n",
    "## Outcome\n",
    "\n",
    "Produces a dataframe of data that suggests variable options based on a user input data dictionary in phase 1.\n",
    "\n",
    "### Output Dataframe structure:\n",
    "\n",
    "First 3 columns are related to a row of user data: \n",
    "\n",
    "user_study | user_var | user_label \n",
    "\n",
    "The next columns are specific to the LLM Output. Cohere will return the top 3 values for each variable and each option will contain the following columns:\n",
    "\n",
    "| option_X_var | option_X_score | option_X_label | option_X_study\n",
    "\n",
    "#### Conditional output scenarios\n",
    "\n",
    "1) If one of the options returns a significantly high match and there are no other close competing options, then we will only return the first value\n",
    "2) If there are no good options we return the options AND prompt our LLM to return an explantion of why the three options are faulty\n",
    "3) If all/multiple options have a close significant match score we return all three options AND query a vector database on which variable's study most closely matches our current user submitted study (TBD)\n",
    "\n",
    "| Column 1 | Column 2 | Column 3 |\n",
    "| -------- | -------- | -------- |\n",
    "| Cell 1A  | Cell 2A  | Cell 3A  |\n",
    "| Cell 1B  | Cell 2B  | Cell 3B  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import cohere\n",
    "\n",
    "from getpass import getpass\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_29', 'id': 361188.0}),\n",
       " Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_15', 'id': 361187.0}),\n",
       " Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_8', 'id': 361189.0}),\n",
       " Document(page_content='Swine H1N1 Vaccination INFO: baseline_variables', metadata={'Study': 'H1N1', 'Variable': 'SWINE_H1N1_VACC', 'id': 208960.0}),\n",
       " Document(page_content='Who is the primary source of information?', metadata={'Study': 'SHEP', 'Variable': 'SH51_INFO_SOURCE', 'id': 407061.0}),\n",
       " Document(page_content='Abstracting for cohort or surveillance Q3', metadata={'Study': 'ARIC', 'Variable': 'HRAA03', 'id': 24922.0}),\n",
       " Document(page_content='Method of Data Collection', metadata={'Study': 'PROPPR', 'Variable': 'DOMOIVTR', 'id': 379440.0}),\n",
       " Document(page_content='Method of Data Collection', metadata={'Study': 'PROPPR', 'Variable': 'DOMOBLDT', 'id': 379439.0}),\n",
       " Document(page_content='COVID-19 specimen #1 type (e.g. nasopharyngeal swab', metadata={'Study': 'REDCORAL', 'Variable': 'CX_COVID1_SPECTYPE', 'id': 384005.0})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_29', 'id': 361188.0}),\n",
    " Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_15', 'id': 361187.0}),\n",
    " Document(page_content='Was information on the COVID Ordinal Outcome Scale', metadata={'Study': 'ORCHID', 'Variable': 'OUT_OOSYN_8', 'id': 361189.0}),\n",
    " Document(page_content='Swine H1N1 Vaccination INFO: baseline_variables', metadata={'Study': 'H1N1', 'Variable': 'SWINE_H1N1_VACC', 'id': 208960.0}),\n",
    " Document(page_content='Who is the primary source of information?', metadata={'Study': 'SHEP', 'Variable': 'SH51_INFO_SOURCE', 'id': 407061.0}),\n",
    " Document(page_content='Abstracting for cohort or surveillance Q3', metadata={'Study': 'ARIC', 'Variable': 'HRAA03', 'id': 24922.0}),\n",
    " Document(page_content='Method of Data Collection', metadata={'Study': 'PROPPR', 'Variable': 'DOMOIVTR', 'id': 379440.0}),\n",
    " Document(page_content='Method of Data Collection', metadata={'Study': 'PROPPR', 'Variable': 'DOMOBLDT', 'id': 379439.0}),\n",
    " Document(page_content='COVID-19 specimen #1 type (e.g. nasopharyngeal swab', metadata={'Study': 'REDCORAL', 'Variable': 'CX_COVID1_SPECTYPE', 'id': 384005.0})]\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_label = \"covid\"\n",
    "# question = f\"Find all variables that match this {variable_label}\"\n",
    "question = f\"From the provided variables, find all that would be helpful in capturing information about {variable_label}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere Reranking\n",
    "\n",
    "This is the start of the data processing post `jupyter/notebooks/search_and_retreival/reranking.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['COHERE_API_KEY']\n",
    "coh = cohere.Client(api_key)\n",
    "\n",
    "rerank_docs = [x.dict()['page_content'] for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example cohere results n 3\n",
    "results = [\n",
    "    {\n",
    "      \"document\": {\n",
    "        \"text\": \"COVID-19 specimen #1 type (e.g. nasopharyngeal swab\"\n",
    "      },\n",
    "      \"index\": 2,\n",
    "      \"relevance_score\": 0.08\n",
    "    },\n",
    "    {\n",
    "      \"document\": {\n",
    "        \"text\": \"Was information on the COVID Ordinal Outcome Scale\"\n",
    "      },\n",
    "      \"index\": 0,\n",
    "      \"relevance_score\":  0.05\n",
    "    },\n",
    "    {\n",
    "      \"document\": {\n",
    "        \"text\": \"Abstracting for cohort or surveillance Q3\"\n",
    "      },\n",
    "      \"index\": 1,\n",
    "      \"relevance_score\":  0.03\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "CohereAPIError",
     "evalue": "invalid api token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCohereAPIError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcoh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrerank_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrerank-multilingual-v2.0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Rank: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Document Index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cohere/client.py:742\u001b[0m, in \u001b[0;36mClient.rerank\u001b[0;34m(self, query, documents, model, top_n, max_chunks_per_doc)\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CohereError(\n\u001b[1;32m    730\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid format for documents, must be a list of strings or dicts with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m key\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    733\u001b[0m json_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: parsed_docs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_chunks_per_doc\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_chunks_per_doc,\n\u001b[1;32m    740\u001b[0m }\n\u001b[0;32m--> 742\u001b[0m reranking \u001b[38;5;241m=\u001b[39m Reranking(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRERANK_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_body\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m reranking\u001b[38;5;241m.\u001b[39mresults:\n\u001b[1;32m    744\u001b[0m     rank\u001b[38;5;241m.\u001b[39mdocument \u001b[38;5;241m=\u001b[39m parsed_docs[rank\u001b[38;5;241m.\u001b[39mindex]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cohere/client.py:929\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, endpoint, json, files, method, stream, params)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m jsonlib\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mJSONDecodeError:  \u001b[38;5;66;03m# CohereAPIError will capture status\u001b[39;00m\n\u001b[1;32m    927\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError\u001b[38;5;241m.\u001b[39mfrom_response(response, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode json body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 929\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cohere/client.py:871\u001b[0m, in \u001b[0;36mClient._check_response\u001b[0;34m(self, json_response, headers, status_code)\u001b[0m\n\u001b[1;32m    869\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-API-Warning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_response:  \u001b[38;5;66;03m# has errors\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    872\u001b[0m         message\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    873\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    874\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    875\u001b[0m     )\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    878\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected client error (status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    880\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    881\u001b[0m     )\n",
      "\u001b[0;31mCohereAPIError\u001b[0m: invalid api token"
     ]
    }
   ],
   "source": [
    "# results = coh.rerank(query=question, documents=rerank_docs, top_n=10, model='rerank-multilingual-v2.0')\n",
    "\n",
    "# for idx, r in enumerate(results):\n",
    "#   print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
    "#   print(f\"Document: {r.document['text']}\")\n",
    "#   print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
    "#   print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the list of dictionaries based on 'relevance_score' in descending order\n",
    "sorted_results = sorted(results, key=lambda x: x['relevance_score'], reverse=True)\n",
    "\n",
    "# Get the top 3 results\n",
    "top_3_results = sorted_results[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Generation in RAG\n",
    "\n",
    "Needed To Dos:\n",
    "- Change the prompt template to more accurately support our end goal\n",
    "  - ... which is?\n",
    "\n",
    "Optional To Dos:\n",
    "- Play around with temperature\n",
    "- Select different OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below I am putting a user query and retrieved context. Read both inputs and make sure to output one score from 0.0 to 1.0 that indicates how relevant the retrieved context is to answer the user query. Query: covid Context: COVID-19 specimen #1 type (e.g. nasopharyngeal swab',\n",
       " 'Below I am putting a user query and retrieved context. Read both inputs and make sure to output one score from 0.0 to 1.0 that indicates how relevant the retrieved context is to answer the user query. Query: covid Context: Was information on the COVID Ordinal Outcome Scale',\n",
       " 'Below I am putting a user query and retrieved context. Read both inputs and make sure to output one score from 0.0 to 1.0 that indicates how relevant the retrieved context is to answer the user query. Query: covid Context: Abstracting for cohort or surveillance Q3']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Below I am putting a user query and retrieved context. Read both inputs and make sure to output one score from 0.0 to 1.0 that indicates how relevant the retrieved context is to answer the user query.\"\n",
    "relevancy_prompt_list = []\n",
    "for i in range(len(top_3_results)):\n",
    "    relevancy_prompt_list.append(f\"{s} Query: {variable_label} Context: {top_3_results[i]['document']['text']}\")\n",
    "relevancy_prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['contexts', 'query'], template=\"\\n    You are a data curator whos role is to harmonize biological variables in the NHLBI (National Heart Lung and Blood)\\n    data repository. You are tasked with evaluating input variables from data dictionary that describes new data that \\n    will be added to the existing pool of variables in the repository. For each new variable, a vector search engine \\n    has returned the three nearest existing variables found in the data repository. \\n\\n    Your job is determine which of the contexts identified by the search, if any, is the best fit for harmonizing the\\n    new variables identified to the contexts. You must explain why the selected context was chosen over the others, \\n    provide information about the relevancy of each context to the new variable.\\n\\n    Then you are to provide the user with as much information as we can on how they can align their new variable with \\n    the selected context.\\n    \\n    When there is no obvious match provide additional information for why you can't make a determination.\\n    \\n    Contexts:\\n    {contexts}\\n\\n    Original question: {query}\")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt V1\n",
    "# We cannot set a seed for OpenAI\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template = \"\"\"\n",
    "    You are a data curator whos role is to harmonize biological variables in the NHLBI (National Heart Lung and Blood)\n",
    "    data repository. You are tasked with evaluating input variables from data dictionary that describes new data that \n",
    "    will be added to the existing pool of variables in the repository. For each new variable, a vector search engine \n",
    "    has returned the three nearest existing variables found in the data repository. \n",
    "\n",
    "    Your job is determine which of the contexts identified by the search, if any, is the best fit for harmonizing the\n",
    "    new variables identified to the contexts. You must explain why the selected context was chosen over the others, \n",
    "    provide information about the relevancy of each context to the new variable.\n",
    "\n",
    "    Then you are to provide the user with as much information as we can on how they can align their new variable with \n",
    "    the selected context.\n",
    "    \n",
    "    When there is no obvious match provide additional information for why you can't make a determination.\n",
    "    \n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Original question: {query}\"\"\",\n",
    ")\n",
    "\n",
    "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19 specimen #1 type (e.g. nasopharyngeal swab\n",
      "---\n",
      "Was information on the COVID Ordinal Outcome Scale\n",
      "---\n",
      "Abstracting for cohort or surveillance Q3\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n---\\n\".join([d['document']['text'] for d in top_3_results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: Based on the provided contexts, it seems that the new variable is related to COVID-19 data. The three nearest existing variables found in the data repository are related to COVID-19 specimen type, COVID Ordinal Outcome Scale, and abstracting for cohort or surveillance.\n",
      "\n",
      "In this case, the best fit for harmonizing the new variable would be the \"COVID-19 specimen #1 type\" context. This is because the new variable likely pertains to a specific type of COVID-19 specimen, such as a nasopharyngeal swab, which aligns closely with this context. The relevance of the other contexts, such as the COVID Ordinal Outcome Scale and abstracting for cohort or surveillance, may not directly match the nature of the new variable.\n",
      "\n",
      "To align the new variable with the selected context of \"COVID-19 specimen #1 type,\" the user should ensure that the data collected for the new variable is specific to the type of specimen being analyzed for COVID-19. They should also follow any standardized protocols or guidelines for categorizing and recording this type of data in the repository.\n",
      "\n",
      "If there is no obvious match between the new variable and any of the provided contexts, it may be necessary to gather more information or consult with domain experts to determine the best approach for harmonizing the variable in the data repository.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'document': {'text': 'COVID-19 specimen #1 type (e.g. nasopharyngeal swab'},\n",
       "  'index': 2,\n",
       "  'relevance_score': 0.08},\n",
       " {'document': {'text': 'Was information on the COVID Ordinal Outcome Scale'},\n",
       "  'index': 0,\n",
       "  'relevance_score': 0.05},\n",
       " {'document': {'text': 'Abstracting for cohort or surveillance Q3'},\n",
       "  'index': 1,\n",
       "  'relevance_score': 0.03}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Relevance Score: {qa_chain(inputs={'query': variable_label, 'contexts': f\"\\n---\\n\".join([d['document']['text'] for d in top_3_results])})['text']}\")\n",
    "\n",
    "top_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'covid',\n",
       " 'contexts': 'COVID-19 specimen #1 type (e.g. nasopharyngeal swab\\n---\\nWas information on the COVID Ordinal Outcome Scale\\n---\\nAbstracting for cohort or surveillance Q3'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs={'query': variable_label, 'contexts': f\"\\n---\\n\".join([d['document']['text'] for d in top_3_results])}\n",
    "\n",
    "inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt V1\n",
    "\n",
    "#### Prompt\n",
    "\n",
    "\"\"\"\n",
    "    Your task is to compare the results of a user query to three retrieved context values.\n",
    "    The user would like to know how closely their question aligns with a context.\n",
    "    Your first step is to select a winning context by comparing the user query to the three contexts.\n",
    "    You should explain your rationale in selecting the winning conext and explain how the user could alter their query to better align with the context.\n",
    "\n",
    "    You should then discuss the other contexts and explain their relevance to the user query and why they were not selected. \n",
    "    \n",
    "    We want to provide the user with as much information as we can on how to\n",
    "    align their variables with the data corpus.\n",
    "    \n",
    "    Contexts:\n",
    "    COVID-19 specimen #1 type (e.g. nasopharyngeal swab\n",
    "    ---\n",
    "    Was information on the COVID Ordinal Outcome Scale\n",
    "    ---\n",
    "    Abstracting for cohort or surveillance Q3\n",
    "\n",
    "    Original question: covid\"\"\"\n",
    "\n",
    "#### Output\n",
    "\n",
    "Relevance Score: In comparing the user query \"covid\" to the three contexts provided, the most relevant context appears to be \"COVID-19 specimen #1 type (e.g. nasopharyngeal swab).\" This is because the user query directly relates to COVID-19, which aligns with the topic of this context. To better align their query with this context, the user could specify their question further, such as asking about the most common type of specimen used for COVID-19 testing or the accuracy of nasopharyngeal swab tests.\n",
    "\n",
    "The context \"Was information on the COVID Ordinal Outcome Scale\" was not selected as the winning context because the user query does not specifically mention outcomes or scales related to COVID-19. However, if the user is interested in understanding the severity or progression of COVID-19 cases, they could modify their query to include terms like \"COVID-19 outcome scale\" or \"severity of COVID-19 cases.\"\n",
    "\n",
    "The context \"Abstracting for cohort or surveillance Q3\" was also not chosen as the winning context because the user query does not indicate a specific interest in cohort studies or surveillance related to COVID-19. If the user is looking for information on COVID-19 surveillance data or cohort studies, they could refine their query to include terms like \"COVID-19 surveillance data\" or \"COVID-19 cohort studies.\"\n",
    "\n",
    "Overall, by providing more specific details in their query, the user can better align their question with the relevant context and receive more accurate and helpful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: COVID-19 specimen #1 type (e.g. nasopharyngeal swab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: Based on the user query \"covid\", the winning context would be \"COVID-19 specimen #1 type (e.g. nasopharyngeal swab)\". This is because the user query directly mentions COVID-19, which aligns closely with this context. The user could alter their query to be more specific by asking about the type of specimen used for COVID-19 testing, such as \"What is the most common specimen type used for COVID-19 testing?\".\n",
      "\n",
      "The other two contexts, which were not selected as the winning context, are still relevant to the user query but to a lesser extent. \n",
      "\n",
      "The second context, \"COVID-19 testing locations\", could be relevant if the user is looking for information on where to get tested for COVID-19. To align their query with this context, the user could ask \"Where can I get tested for COVID-19 near me?\".\n",
      "\n",
      "The third context, \"COVID-19 symptoms\", could also be relevant if the user is looking for information on the symptoms of COVID-19. To align their query with this context, the user could ask \"What are the common symptoms of COVID-19?\".\n",
      "\n",
      "In conclusion, by providing the user with specific examples of how to alter their query to better align with the different contexts, we can help them find the information they are looking for more effectively.\n",
      "\n",
      "\n",
      "Prompt: Was information on the COVID Ordinal Outcome Scale\n",
      "Relevance Score: Based on the user query \"covid,\" the winning context would be \"Was information on the COVID Ordinal Outcome Scale.\" This context directly relates to the topic of COVID, which is what the user query is about. The user could alter their query to be more specific by asking about the COVID Ordinal Outcome Scale to align better with this context.\n",
      "\n",
      "The other two contexts, \"How to bake a chocolate cake\" and \"Best hiking trails in the area,\" were not selected because they are not relevant to the user query about COVID. While these contexts may be interesting or useful in other situations, they do not align with the user's current query.\n",
      "\n",
      "It is important for the user to be specific in their queries to receive the most relevant information. By focusing on the topic they are interested in, such as the COVID Ordinal Outcome Scale, they can ensure that the results they receive are closely aligned with their query.\n",
      "\n",
      "\n",
      "Prompt: Abstracting for cohort or surveillance Q3\n",
      "Relevance Score: Based on the user query \"covid\", the winning context would be \"Abstracting for cohort or surveillance Q3\". This is because the user query directly relates to COVID-19, which is a topic often associated with cohort studies and surveillance efforts. The user could further align their query with this context by specifying if they are looking for information related to COVID-19 surveillance data, cohort studies on COVID-19 patients, or any other specific aspect related to cohorts or surveillance in the context of COVID-19.\n",
      "\n",
      "The other contexts, \"Abstracting for cohort or surveillance Q1\" and \"Abstracting for cohort or surveillance Q2\", were not selected because they may not directly relate to the user query \"covid\". However, if the user query was more specific, such as \"covid surveillance data\" or \"covid cohort study\", these contexts could become more relevant. It is important for the user to provide more specific keywords or details in their query to better align with the different contexts and retrieve more accurate and relevant information.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in top_3_results:\n",
    "    print(f\"Prompt: {prompt['document']['text']}\")\n",
    "    print(f\"Relevance Score: {qa_chain(inputs={'query': variable_label, 'contexts': prompt['document']['text']})['text']}\")\n",
    "    print(\"\\n\")\n",
    "# out = qa_chain(\n",
    "#     inputs={\n",
    "#         \"query\": variable_label,\n",
    "#         \"contexts\": \"\\n---\\n\".join([d['document']['text'] for d in top_3_results])\n",
    "#     }\n",
    "# )\n",
    "# out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt V2 Testing\n",
    "\n",
    "Let's start by using some better test data to see how the LLM performs on test data that is more similar to what we are trying to match to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_label_1 = \"Highest level of formal/academic education achieved\"\n",
    "variable_label_2 = \"lesion MLD and %DS in the persistent region\"\n",
    "variable_label_3 = \"Doing things that make you feel valued\"\n",
    "variable_label_4 = \"Acute Exacerbation, Medical Monitor Identified Even\"\n",
    "variable_label_5 = \"COVID-19 specimen #1 type (e.g. nasopharyngeal swab\"\n",
    "\n",
    "\n",
    "# question = f\"Find all variables that match this {variable_label}\"\n",
    "question = f\"From the provided variable, find all that would be helpful in capturing information about {variable_label}\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
